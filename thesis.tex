\documentclass{mcmthesis}
\mcmsetup{CTeX = true,   % 使用 CTeX 套装时，设置为 true
        tcn = 0000, problem = A,
        sheet = true, titleinsheet = true, keywordsinsheet = true,
        titlepage = false, abstract = true}
\usepackage[UTF8]{ctex}
\usepackage{newtxtext}%\usepackage{palatino}
\usepackage{lipsum}
\usepackage{cite}
\usepackage[nottoc,notlot,notlof]{tocbibind}
\title{MCM Thesis}
\begin{document}
\begin{abstract}
  Abstract here

\cite{dynamics_of_a_new_SIR_epidemic_model}
\cite{sutton1980coelomycetes}
\begin{keywords}
keyword1; keyword2
\end{keywords}
\end{abstract}
\maketitle
%% Generate the Table of Contents, if it's needed.
\tableofcontents
\newpage
%% Generate the Memorandum, if it's needed.
% \memoto{\LaTeX{}studio}
% \memofrom{Liam Huang}
% \memosubject{Happy \TeX{}ing!}
% \memodate{\today}
% % \logo{\LARGE I'm pretending to be a LOGO!}
% \begin{memo}[Memorandum]
%   \lipsum[1-3]
% \end{memo}

\section{Introduction}
\subsection{Problem Restatement}
\subsection{Background Research}
\subsection{Our Work}

\section{Preparation of the Models}
\subsection{Assumptions and Justifications}
\begin{figure}[h]
\small
\centering
\includegraphics[width=8cm]{example-image-a}
\caption{The name of figure} \label{fig:aa}
\end{figure}
\subsection{Notations}

\section{Problem 1: Linear regression model}

The basic concept of linear regression model is that the sum of squares of the difference between the actual sampling and the linear prediction sampling reaches the minimum value, that is , the approximation of the minimum mean square error can determine the unique set of predictor coefficients.\cite{}

\subsection{The fundamentals of linear regression}

In the process of model parameter estimation, the following systems is called linear predictor.

\begin{equation}
  \hat{x}(n)=\sum_{k=1}^{p} \omega_{p}(k) x(n-k)
\end{equation}

Let $\omega_{p}(k)$ be the linear predictive coefficient and $p$ is the predictive order. The prediction error function is

\begin{equation}
  e(n)=x(n)-\hat{x}(n)
\end{equation}

That is

\begin{equation}
  e(n)=x(n)-\sum_{k=1}^{p} \omega_{p}(k) x(n-k)
  \label{alg_a}
\end{equation}

Let

\begin{equation}
  a_{p}(k)=-\omega_{p}(k)
\end{equation}

Then, (\ref{alg_a}) becomes

\begin{equation}
  e(n)=x(n)+\sum_{k=1}^{p} \omega_{p}(k) x(n-k)
  \label{alg_b}
\end{equation}

The mean square of the error is $\varepsilon_{p}=\sum_{n=p}^{N}|e(n)|^{2}$. To minimize the mean square value of the error, that is, the partial derivative is zero.

\begin{equation}
  \frac{\partial \varepsilon_{p}}{\partial a_{p}(k)}=\frac{\partial \sum_{n=p}^{N}|e(n)|^{2}}{\partial a_{p}(k)}=2 \sum_{n=p}^{N}[e(n) x(n-k)]=0
  \label{alg_c}
\end{equation}

That is

\begin{equation}
  \frac{\partial \varepsilon_{p}}{\partial a_{p}(k)}=2 \sum_{n=p}^{N}[e(n) x(n-k)]=0
\end{equation}

Substituting (\ref{alg_b}) into (\ref{alg_c}), we get

\begin{equation}
  \sum_{n=p}^{N}\left\{\left[x(n)+\sum_{k=1}^{p} a_{p}(k) x(n-k)\right] x(n-k)\right\}=\sum_{n=p}^{N} x(n) \cdot x(n-k)+\sum_{k=1}^{p} a_{p}(k) \cdot \sum_{n=p}^{N}[x(n-k) \cdot x(n-k)]
  \label{alg_d}
\end{equation}

Let

\begin{equation}
  \sum_{n=p}^{N} x(n) \cdot x(n-k)+\sum_{k=1}^{p} a_{p}(k) \cdot \sum_{n=p}^{N}[x(n-k) \cdot x(n-k)]=0
\end{equation}

In the finite sampling interval, the autocorrelation function of the real field is defined as

\begin{equation}
  r_{x}(k)=\sum_{n=k}^{N} x(n) \cdot x(n-k)(k=0,1, \cdots, p)
  \label{alg_e}
\end{equation}

Substituting (\ref{alg_e}) into (\ref{alg_d}), we get

\begin{equation}
  r_{x}(k)+\sum_{k=1}^{p} a_{p}(k) r_{x}(n-k)=0
\end{equation}

That is

\begin{equation}
  \sum_{k=1}^{p} a_{p}(k) r_{x}(n-k)=-r_{x}(k)
  \label{alg_f}
\end{equation}

We represent (\ref{alg_f}) as a matrix.

\begin{equation}
  \left(\begin{array}{cccc}
  r_{x}(0) & r_{x}(1) & \cdots & r_{x}(p-1) \\
  r_{x}(1) & r_{x}(0) & \cdots & r_{x}(p-2) \\
  \vdots & \vdots & \ddots & \cdots \\
  r_{x}(p-1) & r_{x}(p-2) & \cdots & r_{x}(0)
  \end{array}\right)\left(\begin{array}{c}
  a_{p}(1) \\
  a_{p}(2) \\
  \vdots \\
  a_{p}(p)
  \end{array}\right)=-\left(\begin{array}{c}
  r_{x}(1) \\
  r_{x}(2) \\
  \vdots \\
  r_{x}(p)
  \end{array}\right)
\end{equation}


\section{Sensitivity Evaluation of the Model}
\section{Model Strengths and Weaknesses}
\subsection{Strengths}
\begin{itemize}
\item \textbf{Applies widely}\\
This system can be used for many types of airplanes, and it also
\end{itemize}
\subsection{Weaknesses}

\section{Future Considerations}
\section{Conclusion}

\bibliography{ref}{}
\bibliographystyle{plain}


\begin{appendices}

\section{Article}

In addition, 

\begin{letter}{Dear, Mr. Alpha Chiang}

its a letter

\vspace{\parskip}

Sincerely yours,

Your friends

\end{letter}
Here are simulation programmes we used in our model as follow.\\
是这样的


\section{Programs}
\textbf{\textcolor[rgb]{0.98,0.00,0.00}{Input Python source:}}
\lstinputlisting[language=Python]{./code/extension_decomposition_temp.py}
\end{appendices}
\end{document}